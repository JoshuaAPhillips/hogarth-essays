{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports and preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/joshua/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "path_to_data = \"./text_chunks.tsv\"\n",
    "stopwords = stopwords.words(\"english\")\n",
    "stopwords.extend([\"the\", \"in\", \"of\"])\n",
    "\n",
    "#print(stopwords)\n",
    "\n",
    "# Open training data\n",
    "training_data = pd.read_csv(path_to_data, sep=\"\\t\", header=0)\n",
    "\n",
    "# Tokenise text and remove stopwords\n",
    "training_data[\"TEXT\"] = training_data[\"TEXT\"].apply(lambda x: word_tokenize(x))\n",
    "training_data[\"TEXT\"] = training_data[\"TEXT\"].apply(lambda x: ' '.join([word for word in x if word.lower() not in stopwords]))\n",
    "\n",
    "#training_data.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, is there an ERNIETopic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3ddad465b4457284f6d82b6c000c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 15:45:06,813 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb405ec55d400abe22cc2c49ddd319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 15:45:22,606 - BERTopic - Embedding - Completed ✓\n",
      "2024-11-15 15:45:22,610 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-11-15 15:45:29,366 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-11-15 15:45:29,372 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-11-15 15:45:29,469 - BERTopic - Cluster - Completed ✓\n",
      "2024-11-15 15:45:29,524 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-11-15 15:45:29,716 - BERTopic - Representation - Completed ✓\n",
      "100%|██████████| 50/50 [00:00<00:00, 779.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare embeddings\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(training_data[\"TEXT\"], show_progress_bar=True)\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# Train BERTopic\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probs = topic_model.fit_transform(training_data[\"TEXT\"])\n",
    "hierarchical_topics = topic_model.hierarchical_topics(training_data[\"TEXT\"])\n",
    "\n",
    "# Save model\n",
    "topic_model.save(\"./output/bertopic_model\", serialization=\"safetensors\", save_ctfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('composition', 0.19144879634682016),\n",
       " ('everything', 0.13094539581058962),\n",
       " ('naturally', 0.09163811222897467),\n",
       " ('different', 0.08567724234208268),\n",
       " ('beginning', 0.08510833571172136),\n",
       " ('time', 0.0774172724691031),\n",
       " ('present', 0.06974068245956487),\n",
       " ('thing', 0.06701065207716213),\n",
       " ('continuous', 0.06051138948966097),\n",
       " ('simply', 0.05867872056659128)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(training_data[\"TEXT\"])\n",
    "\n",
    "similar_topics, similarity = topic_model.find_topics(\"composition\", top_n=5)\n",
    "topic_model.get_topic(similar_topics[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
